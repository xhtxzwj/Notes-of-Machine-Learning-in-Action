# 6.5 SMO算法详解

## 6.5.1 概论

### (一) 概念

支持向量机的学习问题可以形式化为求解凸二次规划问题.   这样的凸二次规划问题具有全局最优解,   并且有许多最优化算法可以用于这一问题的求解.   但是当训练样本容量很大时,   这些算法往往变得非常低效,   以致无法使用.   

所以,   如何高效地实现支持向量机学习就成为一个重要的问题.   目前人们已提出许多快速实现算法.   本节讲述其中的序列最小最优化 (sequential minimal optimization,   SMO)  算法,   这种算法1998年由Platt提出.   

### (二) 目标问题–-软间隔对偶问题

首先,  软间隔的对偶问题前面已经说过了, 也就是 (6.40) 



------

$$
\color{red} \max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \tag{6.40}
$$

$$
\color{red} s.t. \sum_{i=1}^{m} \alpha_{i} y_{i}=0
$$

​                                                                               $\color{red}0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \ldots, m$

------

同时, 对软间隔支持向量机,  KKT条件要求
$$
\color{red} \left\{\begin{array}{l}{\alpha_{i} \geqslant 0, \quad \mu_{i} \geqslant 0} \\ {y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i} \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i}\right)=0} \\ {\xi_{i} \geqslant 0, \quad \mu_{i} \xi_{i}=0}\end{array}\right.\tag{6.41}
$$




接下来, 我们变换一下, 先把max变换为min, 然后把 $\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}$ 用核函数表示为 $K(\boldsymbol x_{i},\boldsymbol x_{j})$ , 关于核函数可参考 6.3.2 小节知识.

那么, 就可以变换为:
$$
\color{red}\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(\boldsymbol x_{i}, \boldsymbol x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \tag{7.98}
$$

$$
\color{red} \text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \tag{7.99}
$$

​	     		$\color{red} 0\leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N \tag{7.100}$

在这个问题中, 变量时拉格朗日乘子,  一个变量 $\alpha_{i}$ 对应一个样本点 $(\boldsymbol x_{i},y_{i})$; 变量的总数等于训练样本容量.

### (三) SMO算法的思路

SMO算法是一种启发式算法,  其基本思路是:  **如果所有变量的解都满足此最优化问题的KKT条件 (Karush-Kuhn-Tucker conditions) ,  那么这个最优化问题的解就得到了**.  **因为KKT条件是该最优化问题的充分必要条件**.   否则,  选择**两个变量**,  **固定其他变量**,  **针对这两个变量构建一个二次规划问题**.   这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解,  因为这会使得原始二次规划问题的目标函数值变得更小.   重要的是,  这时子问题可以通过解析方法求解,  这样就可以大大提高整个算法的计算速度.   子问题有两个变量,  **一个是违反KKT条件最严重的那一个,  另一个由约束条件自动确定**.  如此,  SMO算法将原问题不断分解为子问题并对子问题求解,  进而达到求解原问题的目的.   

注意,   **子问题的两个变量中只有一个是自由变量**,  假设 $\alpha_{1},\alpha_{2}$ 为两个变量,  $\alpha_{3}, \alpha_{4}, \cdots, \alpha_{N}$ 固定,   那么由等式约束 (7.99) 可知:
$$
\alpha_{1}=-y_{1} \sum_{i=2}^{N} \alpha_{i} y_{i}
$$
如果 $\alpha_{2}$ 确定, 那么 $\alpha_{1}$ 也随之确定.  所以子问题中同时更新两个变量.

整个SMO算法包括两个部分:  **求解两个变量二次规划的解析方法**和**选择变量的启发式方法** 



## 6.5.2 两个变量二次规划的求解方法

### (一) 优化问题的改写

 不失一般性,  假设选择的两个变量是 $\alpha_{1},\alpha_{2}$ ,  其他变量 $\alpha_{i}(i=3,4, \cdots, N)$ 是固定的.  于是SMO的最优化问题(7.98) ~ (7.100)的子问题可以写成:  
$$
\color{red} \begin{aligned} \min _{\alpha_{1}, \alpha_{2}} \quad W\left(\alpha_{1}, \alpha_{2}\right)=& \frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\ &-\left(\alpha_{1}+\alpha_{2}\right)+y_{1} \alpha_{1} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i 1}+y_{2} \alpha_{2} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i 2} \end{aligned}\tag{7.101}
$$

$$
\color{red}\text { s.t. } \quad \alpha_{1} y_{1}+\alpha_{2} y_{2}=-\sum_{i=3}^{N} y_{i} \alpha_{i}=\zeta \tag{7.102}
$$

$$
\color{red} 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots,N \tag{7.103}
$$

其中,  $K_{i j}=K\left(x_{i}, x_{j}\right), i, j=1,2, \cdots, N$ ,   $\zeta$ 是常数,  目标函数式 (7.101) 中省略了不含 $\alpha_{1},\alpha_{2}$ 的常数项.

> 注: (7.101)的推导过程
>
> 直接带入即可化简, 可得到结果, 但是需要注意以下两个计算过程:
>
> - 但是要注意一个是 $K_{12}=K_{21}$ , 
> -  $y_{1} \alpha_{1} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i 1}$ =$y_{1} \alpha_{1} \sum_{j=3}^{N} y_{j} \alpha_{i} K_{1 j}$ ,  所以可以合并在一起.



### (二) 约束条件

为了求解两个变量的二次规划问题(7.101)～(7.103),  首先分析约束条件,  然后在此约束条件下求极小.  

由于只有两个变量 $\alpha_{1},\alpha_{2}$ ,  约束可以用二维空间中的图形表示（如图7.8所示）  

![](https://i.loli.net/2019/09/05/n4TCsVNpeHrqLzZ.png)

>  注:  观察约束条件 (7.102) 和 (7.103) , 因为 $y_{1}$ 和 $y_{2}$ 的取值只有{-1,1}, 所以当 $y_{1} \neq y_{2}$ 时, 也就变为图 7.8 左边图, $y_{1} = y_{2}$ 时, 也就时图 7.8 右边图 

不等式约束 (7.103) 使得 $(\alpha_{1},\alpha_{2})$ 在盒子 $[0, C] \times[0, C]$ 内, 等式约束 (7.102) 使 $(\alpha_{1},\alpha_{2})$ 在平行于盒子 $[0, C] \times[0, C]$ 的对角线的直线上.  **因此要求的是目标函数在一条平行于对角线的线段上的最优值**.  这使得两个变量的最优化问题成为**实质上的单变量的最优化问题**,  不妨考虑为变量 $\alpha_{2}$ 的最优化问题.   



假设问题 (7.101)～(7.103) 的初始可行解为 $\alpha_{1}^{\text {old }}, \alpha_{2}^{\text {old }}$ , 最优解为 $\alpha_{1}^{\text {new }}, \alpha_{2}^{\text {new }}$ ,  并且假设在沿着约束方向未经剪辑时 $\alpha_{2}$ 的最优解为 $\alpha^{\text {new,unc}}_{2}$ .

由于 $\alpha_{2}^{\text {new }}$ 需满足不等式约束 (7.103) , 所以最优值 $\alpha_{2}^{\text {new }}$ 的取值范围必须满足条件
$$
L \leqslant \alpha_{2}^{\mathrm{new}} \leqslant H
$$

其中, $L$ 和 $H$ 是 $\alpha_{2}^{\text {new }}$所在的对角线段端点的界.  如果 $y_{1} \neq y_{2}$ , 即图 7.8 左边图, 则有:
$$
L=\max \left(0, \alpha_{2}^{\mathrm{old}}-\alpha_{1}^{\mathrm{old}}\right), \quad H=\min \left(C, C+\alpha_{2}^{\mathrm{old}}-\alpha_{1}^{\mathrm{old}}\right)
$$
如果 $y_{1} = y_{2}$ , 即图 7.8 右边图, 则
$$
L=\max \left(0, \alpha_{2}^{\text {old }}+\alpha_{1}^{\text {old }}-C\right), \quad H=\min \left(C, \alpha_{2}^{\text {old }}+\alpha_{1}^{\text {old }}\right)
$$

> 注:<font color=red> **$L$ 和 $H$ 的推导过程** </font>
>
> 首先根据原问题的约束条件和初始解,最优解有:
> $$
> \begin{array}{l}{\alpha_{1}^{n e w} y_{1}+\alpha_{2}^{n e w} y_{2}=\alpha_{1}^{o l d} y_{1}+\alpha_{2}^{o l d} y_{2}=\zeta} \\ {\quad 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N}\end{array}
> $$
> **第一种情况, 当 $y_{1} \neq y_{2}$ , 即图 7.8 左边图,** 那么有:
> $$
> \alpha_{1}^{o l d}-\alpha_{2}^{o l d}=\alpha_{1}^{new}-\alpha_{2}^{new}=\zeta
> $$
> 进行如下推导:
> $$
> \alpha_{2}^{new}=\alpha_{1}^{new}-(\alpha_{1}^{o l d}-\alpha_{2}^{o l d})\tag{I}
> $$
> 这里需要注意的一点是, $\alpha_{2}^{new}$ 是待求解的, $\alpha_{1}^{new}$ 是变化的.
>
>  又 ${\quad 0 \leqslant \alpha_{2}^{new} \leqslant C}$,  ${\quad 0 \leqslant \alpha_{1}^{new} \leqslant C}$,  那么 $\alpha_{2}^{new}$ 的最小值最小只能到0 , 什么时候取 $0$ 呢, 就是 $(\alpha_{1}^{o l d}-\alpha_{2}^{o l d})<0$ 时, 当 $(\alpha_{1}^{o l d}-\alpha_{2}^{o l d})>0$ 时, 最小值就是 在 $\alpha_{1}^{new}=0$ 时, $I$ 式变为:
>
> $\alpha_{2}^{new}=-(\alpha_{1}^{o l d}-\alpha_{2}^{o l d})$ , 因此, L的取值范围就是 $L=\max \left(0, \alpha_{2}^{\mathrm{old}}-\alpha_{1}^{\mathrm{old}}\right)$
>
> 同理可以求得 $H$ 的取值范围:
> $$
> H=\min \left(C, C+\alpha_{2}^{\mathrm{old}}-\alpha_{1}^{\mathrm{old}}\right)
> $$
> 具体可以参见下图:
>
> ![](https://i.loli.net/2019/09/05/GfsNtClOPkM617D.png)
>
> 
>
> **第二种情况,  如果 $y_{1} = y_{2}$ , 即图 7.8 右边图,** 
>
> 可以根据同样的方法,推导得到 $L$ 和 $H$ 的取值范围:
> $$
> L=\max \left(0, \alpha_{2}^{\text {old }}+\alpha_{1}^{\text {old }}-C\right), \quad H=\min \left(C, \alpha_{2}^{\text {old }}+\alpha_{1}^{\text {old }}\right)
> $$
> 取值范围如下图:
>
> ![](https://i.loli.net/2019/09/05/i1GpmaZRTFN7nqK.png)
>
> 



### (三) 两个变量的解

首先求沿着约束方向未经剪辑**即未考虑不等式约束(7.103)时** $\alpha_{2}$ 的最优解 $\alpha^{\text {new,unc}}_{2}$, 然后再求剪辑后 $\alpha_{2}$ 的解 $\alpha_{2}^{\text {new }}$

为了后面公式的简洁, 记:
$$
g(x)=\sum_{i=1}^{N} \alpha_{i} y_{i} K\left(x_{i}, x\right)+b \tag{7.104}
$$
令
$$
E_{i}=g\left(x_{i}\right)-y_{i}=\left(\sum_{j=1}^{N} \alpha_{j} y_{j} K\left(x_{j}, x_{i}\right)+b\right)-y_{i}, \quad i=1,2 \tag{7.105}
$$
**当 $i=1,2$ 时,  $g(x)$ 为 $x$ 的预测值, $E_{i}$ 为函数 $g(x)$ 对输入 $x_{i}$ 的预测值与真实输出 $y_{i}$ 之差.**



------

<font color=red>**定理 7.6**  两个变量的解</font>

最优化问题 (7.101)～(7.103) 沿着约束方向<font color=red>**未经剪辑时的解是** </font>:
$$
\alpha_{2}^{\text {new, unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left(E_{1}-E_{2}\right)}{\eta} \tag{7.106}
$$
其中,  
$$
\eta=K_{11}+K_{22}-2 K_{12}=\left\|\Phi\left(x_{1}\right)-\Phi\left(x_{2}\right)\right\|^{2} \tag{7.107}
$$
$\Phi\left(x_{1}\right)$ 是输入空间到特征空间的映射,  $E_{i}, \quad i=1,2$ , 由式 (7.105) 给出.

<font color=red>**经剪辑后 $\alpha_{2}$ 的解**</font>是
$$
\alpha_{2}^{\mathrm{new}}=\left\{\begin{array}{ll}{H,} & {\alpha_{2}^{\mathrm{new}, \mathrm{unc}}>H} \\ {\alpha_{2}^{\mathrm{new}, \mathrm{unc}},} & {L \leqslant \alpha_{2}^{\mathrm{new}, \mathrm{unc}} \leqslant H} \\ {L,} & {\alpha_{2}^{\mathrm{new}, \mathrm{unc}}<L}\end{array}\right. \tag{7.108}
$$
由 $\alpha_{2}^{\text {new }}$ 求得 $\alpha_{1}^{\text {new }}$是:
$$
\alpha_{1}^{\mathrm{new}}=\alpha_{1}^{\mathrm{old}}+y_{1} y_{2}\left(\alpha_{2}^{\mathrm{old}}-\alpha_{2}^{\mathrm{new}}\right) \tag{7.109}
$$

------

> **注: 关于定理的推导过程.**
>
> 1. 关于**未经剪辑时的解的推导**过程:
>
>    请参考李航<统计学习方法> p127-128的证明
>
> 2. 经剪辑后的解 (7.108) 的解释:
>
>    要使其满足不等式约束必须将其限制在区间 $[L,  H]$内,  从而得到 $\alpha_{2}^{\text {new }}$ 的表达式 (7.108)
>
> 3.  $\alpha_{1}^{\text {new }}$ 的解 (7.109) 的解释:
>
>    由等式约束 (7.102) , 得到  $\alpha_{1}^{\text {new }}$ 的表达式 (7.109) 



## 6.5.3 变量的选择方法

SMO算法在每个子问题中选择两个变量优化,  其中至少一个变量是违反KKT条件的.

### (一) 第 1 个变量的选择

SMO称选择第1个变量的过程为外层循环.  外层循环在训练样本中选取违反 KKT 条件最严重的样本点,  并将其对应的变量作为第1个变量.  具体地,  检验训练样本点 $\left(x_{i}, y_{i}\right)$ 是否满足KKT条件,  即
$$
\alpha_{i}=0 \Leftrightarrow y_{i} g\left(x_{i}\right) \geqslant 1 \tag{7.111}
$$

$$
0<\alpha_{i}<C \Leftrightarrow y_{i} g\left(x_{i}\right)=1 \tag{7.112}
$$

$$
\alpha_{i}=C \Leftrightarrow y_{i} g\left(x_{i}\right) \leqslant 1 \tag{7.113}
$$

其中,  $g\left(x_{i}\right)=\sum_{j=1}^{N} \alpha_{j} y_{j} K\left(x_{i}, x_{j}\right)+b$ .

> 注1 : 关于 (7.111)~(7.113) 的推导:
>
> 1. $\alpha_{i}=0$ 
>
>    由(6.39) 知: $C=\alpha_{i}+\mu_{i}$ , 可得:
>    $$
>    \mu_{i}=C
>    $$
>    再由对偶问题的 kkt 条件 (6.41) 中的 $ \mu_{i} \xi_{i}=0$ 可知:
>    $$
>    \xi_{i}=0
>    $$
>    再由 kkt 条件中的 ${y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i} \geqslant 0}$ (或者原始问题的约束条件, 是一样的), 有:
>    $$
>    y_{i} g\left(x_{i}\right) \geqslant 1
>    $$
>
> 2. $0<\alpha_{i}<C$
>
>    若  $\alpha_{i}>0$ ,  则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1-\xi_{i}$ ,  即该样本是支持向量, 由式 (6.39), 即 $C=\alpha_{i}+\mu_{i}$ 可知, 若 $\alpha_{i}<C$ ,  则 $\mu_{i}>0$ ,  根据 $ \mu_{i} \xi_{i}=0$ ,  进而有 $\xi_{i}=0$ ,  即该样本恰在最大间隔边界上; 所以有, $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ , 即 $y_{i} g\left(x_{i}\right)=1$
>
> 3. $\alpha_{i}=C$ 
>
>    首先,  $\xi_{i}\geqslant0$ , 同时, 由于 $\alpha_{i}=C$ , 那么由 $\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i}\right)=0$ 可得,  $\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i}\right)=0$, 所以 $y_{i} f(\boldsymbol{x}_{i})\leqslant1$



> 注2: 其实  (7.111)~(7.113) 就是 kkt 条件 (6.41) 的充要条件, 两者可以互相推出.



检验是在精度 $\varepsilon$ 范围内进行的.  在检验过程中,  外层循环首先遍历所有满足条件 $0<\alpha_{i}<C$ 的样本点,  即在间隔边界上的支持向量点,  检验它们是否满足KKT条件.  如果这些样本点都满足KKT条件,  那么遍历整个训练集,  检验它们是否满足KKT条件.  



### (二) 第 2 个变量的选择

SMO称选择第2个变量的过程为内层循环.  假设在外层循环中已经找到第1个变量 $\alpha_{1}$ ,  现在要在内层循环中找第2个变量 $\alpha_{2}$ .  第2个变量选择的标准是希望能使 $\alpha_{2}$ 有足够大的变化.  由式 (7.106) 和式(7.108) 可知,   是依赖于$|E_{1}-E_{2}|$ 的,   为了加快计算速度,   一种简单的做法是选择 $\alpha_{2}$ ,   使其对应的 $|E_{1}-E_{2}|$ 最大.  因为 $\alpha_{1}$ 已定,   $E_{1}$ 也确定了.  如果 $E_{1}$ 是正的,  那么选择最小的 $E_{i}$ 作为$E_{2}$;  如果 $E_{1}$ 是负的,   那么选择最大的 $E_{i}$作为$E_{2}$.  为了节省计算时间,   将所有 $E_{i}$ 值保存在一个列表中.  在特殊情况下,   如果内层循环通过以上方法选择的 $\alpha_{2}$ 不能使目标函数有足够的下降,  那么采用以下启发式规则继续选择 $\alpha_{2}$ .  遍历在间隔边界上的支持向量点,   依次将其对应的变量作为 $\alpha_{2}$ 试用,   直到目标函数有足够的下降.  若找不到合适的 $\alpha_{2}$ ,   那么遍历训练数据集;  若仍找不到合适的 $\alpha_{2}$ ,   则放弃第1个 $\alpha_{1}$ ,   再通过外层循环寻求另外的 $\alpha_{1}$  



### (三) 计算阈值 $b$ 和差值 $E_{i}$ 

在每次完成两个变量的优化后,  都要重新计算阈值b。 当 $0<\alpha_{1}^{\mathrm{new}}<\mathrm{C}$ 时,  由 KKT 条件 (7.112) 可知: 
$$
\sum_{i=1}^{N} \alpha_{i} y_{i} K_{i 1}+b=y_{1}
$$

> 注:  $\left(x_{1}, y_{1}\right)$ 也满足(7.112), 两边同乘以 $y_{1}$ , 有:
> $$
> y^{2}_{1} g\left(x_{i}\right)=y_{1}
> $$
> 又 $y^{2}_{1}=1$ , 即可得到上述结论



于是, 可得:
$$
b_{1}^{\mathrm{new}}=y_{1}-\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}-\alpha_{1}^{\mathrm{new}} y_{1} K_{11}-\alpha_{2}^{\mathrm{new}} y_{2} K_{21} \tag{7.114}
$$
由 $E_{1}$ 的定义式 (7.105) 有:
$$
E_{1}=\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}+\alpha_{1}^{\mathrm{old}} y_{1} K_{11}+\alpha_{2}^{\mathrm{old}} y_{2} K_{21}+b^{\mathrm{old}}-y_{1}
$$
式 (7.114) 的前两项可以通过 $E_{1}$ 改写为:
$$
y_{1}-\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}=-E_{1}+\alpha_{1}^{\mathrm{old}} y_{1} K_{11}+\alpha_{2}^{\mathrm{old}} y_{2} K_{21}+b^{\mathrm{old}} 
$$
带入式 (7.114) , 可得:
$$
b_{1}^{\text {new }}=-E_{1}-y_{1} K_{11}\left(\alpha_{1}^{\text {new }}-\alpha_{1}^{\text {eld }}\right)-y_{2} K_{21}\left(\alpha_{2}^{\text {new }}-\alpha_{2}^{\text {old }}\right)+b^{\text {old }} \tag{7.115}
$$
那么, 同样的, 如果 $0<\alpha_{2}^{\mathrm{new}}<C$ , 则有:
$$
b_{2}^{\text {new }}=-E_{2}-y_{1} K_{12}\left(\alpha_{1}^{\text {new }}-\alpha_{1}^{\text {old }}\right)-y_{2} K_{22}\left(\alpha_{2}^{\text {new }}-\alpha_{2}^{\text {old }}\right)+b^{\text {old }} \tag{7.116}
$$


- **如果 $\alpha_{1}^{\mathrm{new}}$ , $\alpha_{2}^{\mathrm{new}}$ 同时满足条件 $0<\alpha_{i}^{\mathrm{new}}<C, i=1,2$ (也就是 $b_{1}^{\text {new }}$ 和 $b_{2}^{\text {new }}$ 都有效的时候), 他们是相等的, 即  $b^{\text {new }}=b_{1}^{\text {new }}=  b_{2}^{\text {new }}$** 
- **如果 $\alpha_{1}^{\mathrm{new}}$ , $\alpha_{2}^{\mathrm{new}}$ 是 0 或者 C , 那么  $b_{1}^{\text {new }}$ 和 $b_{2}^{\text {new }}$ 以及他们两者之间的数都是符合 KKT 条件的阈值, 这时选择它们的中点作为 $b^{n e w}=\frac{b_{1}^{n e w}+b_{2}^{n e w}}{2}$**





### 6.5.4 SMO算法总结

------

<font color=red>**算法 7.5 (SMO算法)** </font>

**输入:** 训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$ , 其中, $x_{i} \in \mathcal{X}=\mathbf{R}^{n}$ , $y_{i} \in \mathcal{Y}=\{-1,+1\}$, $\quad i=1,2, \cdots, N$ , 精度 $\mathcal{E}$ 

**输出:** 近似解 $\hat{\alpha}$ 

- (1) 取初值 $\alpha^{(0)}=0$ ,  令 $k=0$ ;

- (2) 按照 6.5.3 变量的选择方法中第一个变量选择, 选择第一个变量 $\alpha_{1}^{(k)}$ , 按照第二个变量选择方法选择第二个变量 $\alpha_{2}^{(k)}$ ,  根据式 (7.106) , 求出新的 $\alpha_{2}^{\text {new, unc }}$ , 
  $$
  \alpha_{2}^{\text {new, unc }}=\alpha_{2}^{(k)}+\frac{y_{2}\left(E_{1}-E_{2}\right)}{\eta}
  $$

- (3) 按照下式 (即式 (7.108) ) 求出 $\alpha_{2}^{(k+1)}$ 
  $$
  \alpha_{2}^{(k+1)}=\left\{\begin{array}{ll}{H,} & {\alpha_{2}^{\mathrm{new}, \mathrm{unc}}>H} \\ {\alpha_{2}^{\mathrm{new}, \mathrm{unc}},} & {L \leqslant \alpha_{2}^{\mathrm{new}, \mathrm{unc}} \leqslant H} \\ {L,} & {\alpha_{2}^{\mathrm{new}, \mathrm{unc}}<L}\end{array}\right.
  $$

- (4) 利用  $\alpha_{2}^{(k+1)}$ 和  $\alpha_{1}^{(k+1)}$ 的关系(即式 (7.109) ) , 求出   $\alpha_{1}^{(k+1)}$ .
  $$
  \alpha_{1}^{(k+1)}=\alpha_{1}^{(k)}+y_{1} y_{2}\left(\alpha_{2}^{(k)}-\alpha_{2}^{(k+1)}\right)
  $$

- (5) 按照 6.5.3 变量的选择方法中的 (三) 计算阈值 $b$ 和差值 $E_{i}$ , 计算 $b^{k+1}$ 和 $E_{i}$ 

- (6) 在精度 $\mathcal{E}$ 范围内检查是否满足如下的终止条件:
  $$
  \sum_{i=1}^{N} \alpha_{i} y_{i}=0
  $$

  $$
  0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
  $$

  $$
  \alpha_{i}^{k+1}=0 \Rightarrow y_{i} g\left(x_{i}\right) \geq 1
  $$

  $$
  0<\alpha_{i}^{k+1}<C \Rightarrow y_{i} g\left(x_{i}\right)=1
  $$

  $$
  \alpha_{i}^{k+1}=C \Rightarrow y_{i} g\left(x_{i}\right) \leq 1
  $$

- (7) 如果满足则结束, 返回 $\alpha_{i}^{k+1}$, 否则转到步骤 (2) 

------









































