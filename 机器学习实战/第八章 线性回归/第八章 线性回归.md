<center><font size=13>第八章 线性回归</font></center>
# 8.1 线性回归的理论基础

## 8.1.1 基本形式

### 一 定义

给定由 $d$ 个属性描述的示例 $\boldsymbol{x}=\left(x_{1} ; x_{2} ; \ldots ; x_{d}\right)$ , 其中 $x_{i}$ 是 $\boldsymbol{x}$ 在第 $i$ 个属性上的取值, 线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数,即
$$
f(\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{d} x_{d}+b \tag {3.1}
$$
向量形式:
$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \tag{3.2}
$$
其中 $\boldsymbol{w}=\left(w_{1} ; w_{2} ; \ldots ; w_{d}\right)$ . $ \boldsymbol{w}$ 和 $b$ 学得之后, 模型就能确定下来.

### 二 线性模型的意义

线性模型是基础, .许多功能更为强大的非线性模型 (nonlinear model)可在线性模型的基础上
通过引入**层级结构**或**高维映射**而得. 





## 8.1.2 线性回归

给定数据集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$,   其中 $\boldsymbol{x}_{i}=\left(x_{i 1} ; x_{i 2} ; \ldots ; x_{i d}\right), y_{i} \in \mathbb{R}$ . "线性回归" (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记. 

### 一 简单形式的线性回归

最简单的情形:输入属性的数目只有一个.此时忽略关于属性的下标 ,即 $D=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{m}$ ,其中 $x_{i} \in \mathbb{R}$ .

线性回归试图学得
$$
f\left(x_{i}\right)=w x_{i}+b, 使得 f\left(x_{i}\right) \simeq y_{i} \tag{3.3}
$$

#### 1 属性的转化

对离散属性,  若属性值间**存在"序" (order)关系**,  可通过连续化将其转化为**连续值**."身高"的取值"高" "矮"可转化为 $\{1.0,0.0\}$."高" "中" "低"可转化为 $\{1.0,0.5,0.0\}$; 若属性值间**不存在序关系**,  假定有 k 个属性值,  则通常转化为 **k 维向量**,  如属性"瓜类"的取值"西瓜" "南瓜" "黄瓜"可转化为 $(0,0,1),(0,1,0),(1,0,0)$

#### 2 均方误差最小化

试图让均方误差最小化,  即
$$
\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\ &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned} \tag {3.4}
$$
对3.4分别对$w$ 和 $b$ 求导,得到
$$
\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right) \tag {3.5}
$$

$$
\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) \tag{3.6}
$$

最后求得最优的闭式(closed-form)解
$$
w=\frac{\sum\limits_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum\limits_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum\limits_{i=1}^{m} x_{i}\right)^{2}} \tag{3.7}
$$

$$
b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \tag{3.8}
$$

其中 $\overline{x}=\frac{1}{m} \sum_{i=1}^{m} x_{i}$ 为 $x$ 的均值.

> **推导1:**
> $$
> w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}=0 \tag{1}
> $$
>
> $$
> m b=\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \tag{2}
> $$
>
> 由2可以直接求得 $b$ , 再将2带入到1中,有:
> $$
> w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) x_{i}=0
> $$
>
> $$
> w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} y_{i}+\frac{1}{m} \sum_{i=1}^{m} \sum_{i=1}^{m} x_{i}\left(y_{i}-w x_{i}\right)=0
> $$
>
> $$
> w \sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m} \sum_{i=1}^{m} x_{i} \cdot \sum_{i=1}^{m} x_{i} \cdot w+\frac{1}{m} \sum_{i=1}^{m} x_{i} \cdot \sum_{i=1}^{m} y_{i}-\sum_{i=1}^{m} x_{i} y_{i}
> $$
>
> $$
> w\left(\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}\right)=\sum_{i=1}^{m} x_{i} y_{i}-\overline{x} \cdot \sum_{i=1}^{m} y_{i}
> $$
>
> 则就可以求得:
> $$
> w=\frac{\sum\limits_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum\limits_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum\limits_{i=1}^{m} x_{i}\right)^{2}}
> $$



### 二 一般形式

更一般的情形是如本节开头的数据集 $D$ ,   样本由 $d$ 个属性描述.此时我们试图学得 
$$
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b, 使得f\left(\boldsymbol{x}_{i}\right) \simeq y_{i},
$$

> **注1**: 这里的 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{w}$ 是**等价**的.

这也就是"多元线性回归"(multivariate linear regression)

#### 1 形式变换

为了简便起见, 把 $\boldsymbol{w}$ 和 $b$ 吸收乳向量形式 $\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)$ , 相应的, 把数据集 $D$ 表示为一个 $m \times(d+1)$ 大小的矩阵 $\mathbf{X}$ , 其中每行对应于一个示例,  该行前 $d$ 个元素对应于示例的 $d$ 个属性值,  最后一个元素恒置为 1 ,  即 
$$
\mathbf{X}=\left( \begin{array}{ccccc}{x_{11}} & {x_{12}} & {\ldots} & {x_{1 d}} & {1} \\ {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} & {1} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\vdots} \\ {x_{m 1}} & {x_{m 2}} & {\dots} & {x_{m d}} & {1}\end{array}\right)=\left( \begin{array}{cc}{\boldsymbol{x}_{1}^{\mathrm{T}}} & {1} \\ {\boldsymbol{x}_{2}^{\mathrm{T}}} & {1} \\ {\vdots} & {\vdots} \\ {\boldsymbol{x}_{m}^{\mathrm{T}}} & {1}\end{array}\right)
$$
再把标记也写成向量形式 $\boldsymbol{y}=\left(y_{1} ; y_{2} ; \ldots ; y_{m}\right)$ , 则类似于式(3.4), 有
$$
\hat{\boldsymbol{w}}^{*}=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}) \tag{3.9}
$$

> **注2**:  3.9就是向量的平方形式. 同时类似于注1, $\mathbf{X} \hat{\boldsymbol{w}}$ 展开其中一项, 其实就是 $\boldsymbol{x}_{1}^{\mathrm{T}} \boldsymbol{w}+b$

#### 2 求解过程

令 $E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$ , 对 $\hat{\boldsymbol{w}}$ 求导得到
$$
\frac{\partial E_{\hat{\mathbf{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y}) \tag{3.10}
$$

> **推导2**: 将 $E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$ 展开可以得到:
>
> $$E_{\hat{\boldsymbol{w}}}=\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \boldsymbol{\hat { w }}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}} \hat{\boldsymbol{w}}$$ ,再对$ \hat{\boldsymbol{w}}$ 进行求导:
> $$
> \frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=\frac{\partial \boldsymbol{y}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}
> $$
> 根据向量求导公式可得:
>
> $\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=0-\mathbf{X}^{T} \boldsymbol{y}-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{\boldsymbol{w}}$
>
> 进一步得到:
>
> $\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})$

> **注3**: 矩阵求导相关参考资料
>
> [矩阵求导-维基百科](<https://en.wikipedia.org/wiki/Matrix_calculus>)



进一步, 当 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为满秩矩阵或正定矩阵时, 令(3.10) 为零可以得到:
$$
\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y} \tag{3.11}
$$
令 $\hat{\boldsymbol{x}}_{i}=\left(\boldsymbol{x}_{i}, 1\right)$ ,则最终的回归模型为:
$$
f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{x}}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y} \tag{3.12}
$$
而现实任务中 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 常常不是满秩矩阵, 常见的作法是引入正则化(regularization)项.

#### 3 广义线性模型和联系函数

一般的, 考虑单调可微函数 $g(\cdot)$ , 令
$$
y=g^{-1}\left(w^{T} x+b\right) \tag{3.15}
$$
这样的模型称为**"广义线性模型"**(generalized linear model), 其中函数 $g(\cdot)$ 称为**"联系函数"**(link function). 易见, 对数线性回归是广义线性模型在 $g(\cdot)=\ln (\cdot)$ 时的特例. 



# 8.2 线性回归实战

## 8.2.1 线性回归的开发流程

- 收集数据: 采用任意方法收集数据
- 准备数据: 回归需要数值型数据,  标称型数据将被转换成二值型数据
- 分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析,  在采用缩减法求得新回归系数之后,  可以将新拟合线绘在图上作为对比
- 训练算法: 找到回归系数
- 测试算法: 使用 $R^2$ 或者预测值和数据的拟合度,  来分析模型的效果
- 使用算法: 使用回归,  可以在给定输入的时候预测出一个数值,  这是对分类方法的提升,  因为这样可以预测连续型数据而不仅仅是离散的类别标签

## 8.2.2 线性回归算法的特点

- 优点: 结果易于理解,  计算上不复杂. 
- 缺点: 对非线性的数据拟合不好. 
- 适用于数据类型: 数值型和标称型数据. 



## 8.2.3 线性回归实战项目

### 一 简单线性回归项目



#### 1 简单线性回归的工作原理

- 读入数据,  将数据特征 $x$ 和特征标签 $y$ 存储在矩阵 $x、y$ 中
- 验证 $x^Tx$ 矩阵是否可逆
- 使用最小二乘法求得回归系数 $w$ 的最佳估计



#### 2 简单线性回归项目实战

先看下数据ex0.txt的数据样式, 如下:

![](https://i.loli.net/2019/09/30/ar9lDOtV7cXEoSh.png)

> 注: 第一列就是x0, 全为1(原因就是我们的公式中的X第一列为了计算方便, 设为1). 第二列x1也就是x, 第三列x3也就是y



编写的具体代码如下:

```python
import numpy as np
import matplotlib.pyplot as plt


"""
函数说明: 加载并解析数据
Parameters:
    filename - 文件名
Returns:
    dataMat - 数据矩阵
    labelMat - 数据标签
"""

def loadDataSet(fileName):
    numFeat = len((open(fileName).readline().split('\t')))
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat

"""
函数说明: 求解系数矩阵w
Parameters:
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
Returns:
    ws - 回归系数
"""
def standRegres(xArr, yArr):
    #转换成矩阵
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    #计算x^T*x
    xTx = xMat.T*xMat
    #判断xTx是否可逆, 如果不可逆则无法计算
    #linalg.det()计算行列式
    if np.linalg.det(xTx) == 0:
        print('矩阵不可逆,无法求解逆矩阵')
        return
    #利用公式,计算w,xTx.I计算xTx的逆矩阵
    ws = xTx.I*(xMat.T*yMat)
    return ws

"""
函数说明: 绘制回归曲线和数据点
Parameters:
    无
Returns:
    无
"""
def plotRegression(xArr, yArr):
    ws = standRegres(xArr, yArr)
    xMat = np.mat(xArr)
    yMat = np.mat(yArr)
    xCopy = xMat.copy()
    xCopy.sort(0)
    yHat = xCopy * ws
    #关于matplotlip的绘图基础, 参见<利用python进行数据分析>(第二版)
    #里面讲的非常好
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(xCopy[:,1], yHat, c = 'red')
    #flatten是numpy.ndarray.flatten的一个函数,  即返回一个折叠成一维的数组.  
    #但是该函数只能适用于numpy对象,  即array或者mat,  普通的list列表是不行的
    ax.scatter(xMat[:, 1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = 0.5)
    plt.title('DataSet')
    plt.xlabel('X')
    plt.show()
    print(ws)


if __name__ == '__main__':
    xArr, yArr = loadDataSet('Ch08\ex0.txt')
    plotRegression(xArr, yArr)


```

看下运行结果

![](https://i.loli.net/2019/09/30/uvhEl1QTUyOFdj8.png)

求解的ws,如下:

![](https://i.loli.net/2019/09/30/oLAjrincEbCX8Nz.png)



几乎任一数据集都可以用上述方法建立模型,  那么,  如何判断这些模型的好坏呢?  比较一下图8-3的两个子图,  如果在两个数据集上分别作线性回归,  将得到完全一样的模型 (拟合直线) .  显然两个数据是不一样的,  那么模型分别在二者上的效果如何? 我们当如何比较这些效果的好坏呢?  有种方法可以计算预测值yHat序列和真实值y序列的匹配程度,  那就是计算这两个序列的相关系数.  

![](https://i.loli.net/2019/09/30/QOMxXsY59ftFA2b.png)

把`if __name__ == ‘__main__:’ `改动一下即可计算相关系数. 具体如下:

```python
if __name__ == '__main__':
    xArr, yArr = loadDataSet('Ch08\ex0.txt')
    ws = standRegres(xArr, yArr)
    xMat = np.mat(xArr)
    yMat = np.mat(yArr)                                                   
    yHat = xMat * ws
    print(np.corrcoef(yHat.T, yMat))
```

结果如下:

![](https://i.loli.net/2019/09/30/jXFGnpJwR6A4bZW.png)

可以看到,  对角线上的数据是1.0,  因为yMat和自己的匹配是完全匹配的,  而YHat和yMat的相关系数为0.98, 效果也非常好. 



### 二 局部加权线性回归

#### 1 局部加权线性回归的基本理论

线性回归的一个问题是有可能出现欠拟合现象,  因为它求的是具有最小均方误差的无偏估计.  显而易见,  如果模型欠拟合将不能取得最好的预测效果.  所以有些方法允许在估计中引入一些偏差,  从而降低预测的均方误差.   

其中的一个方法是局部加权线性回归 (Locally Weighted Linear Regression,   LWLR) ,  在该算法中,  我们给待预测点附近的每个点赋予一定的权重；然后与8.1节类似,  在这个子集上基于最小均方差来进行普通的回归,  与kNN一样,  这种算法每次预测均需要事先选取出对应的数据子集,  该算法解出回归系数w的形式如下： 
$$
\hat{w}=\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{W} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W} y
$$
其中, $\boldsymbol{W}$ 是一个矩阵,  用来给每个数据点赋予权重.

LWLR使用“核” (与支持向量机中的核类似) 来对附近的点赋予更高的权重,  核的类型可以自由选择,  最常用的核就是高斯核,  高斯核对应的权重如下:
$$
w(i, i)=\exp \left(\frac{\left|x^{(i)}-x\right|}{-2 k^{2}}\right)
$$
这样就构建了一个只含对角元素的权重矩阵 $\boldsymbol{W}$ ,  并且点 $x$ 与 $x(i)$ 越近,   $w(i,i)$ 将会越大,  上述公式包含一个需要用户指定的参数 $k$ ,  它决定了对附近的点赋予多大的权重,  这也是使用LWLR时唯一需要考虑的参数,  在图8-4中可以看到参数k与权重的关系,   

![](https://i.loli.net/2019/09/30/kzbtLwEeQWM2aoB.png)

#### 2 局部加权线性回归的工作原理

- 读入数据,  将数据特征  和特征标签 $y$ 存储在矩阵 $x、y$ 中
- 利用高斯核构造一个权重矩阵 $\boldsymbol{W}$ ,  对预测点附近的点施加权重
- 验证 $\boldsymbol{X}^{\mathrm{T}} \boldsymbol{W} \boldsymbol{X}$ 矩阵是否可逆
- 使用最小二乘法求得回归系数 $w$ 的最佳估计



#### 3 局部加权线性回归项目实战

数据还是上面的ex0.txt数据.

具体实现的代码如下:

```python
import numpy as np
import matplotlib.pyplot as plt


"""
函数说明: 加载并解析数据
Parameters:
    filename - 文件名
Returns:
    dataMat - 数据矩阵
    labelMat - 数据标签
"""

def loadDataSet(fileName):
    numFeat = len((open(fileName).readline().split('\t')))
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat


"""
函数说明: 使用局部加权线性回归计算回归系数w
            局部加权线性回归,  在待预测点附近的每个点赋予一定的权重,  在子集上基于最小均方差来进行普通的回归
Parameters:
    testPoit - 测试样本点
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
    k - 高斯核的k
Returns:
    testPoint * ws - 数据点与具有权重的系数相乘得到的预测点
"""
def lwlr(testPoint, xArr, yArr, k = 1):
    #转换成矩阵
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    #获取xMat矩阵的行数
    m = np.shape(xMat)[0]
    #初始化权重(为对角矩阵)
    #eye()返回一个对角线元素为1,  其他元素为0的二维数组,  创建权重矩阵weights,  该矩阵为每个样本点初始化了一个权重
    weights = np.mat(np.eye((m)))
    #遍历数据集, 计算每个样本的权重
    for j in range(m):
        #testPoint 的形式是 一个行向量的形式
        #计算 testPoint 与输入样本点之间的距离,  然后下面计算出每个样本贡献误差的权值
        diffMat = testPoint - xMat[j, :]
        weights[j,j] = np.exp(diffMat*diffMat.T/ (-2*k**2))
    xTx = xMat.T * (weights * xMat)
    if np.linalg.det(xTx) == 0:
        print("矩阵不可逆")
        return
    #计算出回归系数ws
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws

"""
函数说明: 局部加权线性回归测试
Parameters:
    testArr - 测试数据集
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
    k - 高斯核的k
Returns:
    yHat - 预测值
"""
def lwlrTest(testArr, xArr, yArr, k = 1):
    #得到样本点总数
    m = np.shape(testArr)[0]
    #初始化yHat, 构建一个全部是 0 的 1 * m 矩阵
    yHat = np.zeros(m)
    #循环所有数据点, 并应用lwlr函数
    for i in range(m):
        yHat[i] = lwlr(testArr[i], xArr, yArr, k)
    return yHat

def plotlwlrRegression():
    xArr, yArr = loadDataSet('CH08\ex0.txt')
    #加载数据集
    yHat_1 = lwlrTest(xArr, xArr, yArr, 1.0)
    #根据局部加权线性回归计算yHat
    yHat_2 = lwlrTest(xArr, xArr, yArr, 0.01)
    #根据局部加权线性回归计算yHat
    yHat_3 = lwlrTest(xArr, xArr, yArr, 0.003)
    #根据局部加权线性回归计算yHat
    xMat = np.mat(xArr)
    #创建xMat矩阵
    yMat = np.mat(yArr)
    #创建yMat矩阵
    srtInd = xMat[:, 1].argsort(0)
    #排序,  返回索引值
    xSort = xMat[srtInd][:,0,:]
    fig, axs = plt.subplots(nrows=3, ncols=1,sharex=False, sharey=False, figsize=(10,8))
    axs[0].plot(xSort[:, 1], yHat_1[srtInd], c = 'red')
    #绘制回归曲线
    axs[1].plot(xSort[:, 1], yHat_2[srtInd], c = 'red')
    #绘制回归曲线
    axs[2].plot(xSort[:, 1], yHat_3[srtInd], c = 'red')
    #绘制回归曲线
    axs[0].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)
    #绘制样本点
    axs[1].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)
    #绘制样本点
    axs[2].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)
    #绘制样本点
    #设置标题,x轴label,y轴label
    axs0_title_text = axs[0].set_title(u'局部加权回归曲线,k=1.0')
    axs1_title_text = axs[1].set_title(u'局部加权回归曲线,k=0.01')
    axs2_title_text = axs[2].set_title(u'局部加权回归曲线,k=0.003')
    plt.setp(axs0_title_text, size=8, weight='bold', color='red')
    plt.setp(axs1_title_text, size=8, weight='bold', color='red')
    plt.setp(axs2_title_text, size=8, weight='bold', color='red')
    plt.xlabel('X')
    plt.show()



if __name__ == '__main__':
    plotlwlrRegression()

```

结果如下:

![](https://i.loli.net/2019/09/30/VSu7lXM5oDp3nqN.png)

上图给出了k在三种不同取值下的结果图.  当k = 1.0时权重很大,  如同将所有的数据视为等权重,  得出的最佳拟合直线与标准的回归一致.  使用k = 0.01得到了非常好的效果,  抓住了数据的潜在模式.  使用k = 0.003纳入了太多的噪声点,  拟合的直线与数据点过于贴近.  所以,  **上图中的最下图是过拟合的一个例子,  而最上图则是欠拟合的一个例子.**

### 三 示例: 预测鲍鱼的年龄

有一份来自 UCI 的数据集合的数据,  记录了鲍鱼 (一种介壳类水生动物) 的年龄.  鲍鱼年龄可以从鲍鱼壳的层数推算得到.

先看下数据格式:



具体代码如下:

```python
import numpy as np
import matplotlib.pyplot as plt


"""
函数说明: 加载并解析数据
Parameters:
    filename - 文件名
Returns:
    dataMat - 数据矩阵
    labelMat - 数据标签
"""

def loadDataSet(fileName):
    numFeat = len((open(fileName).readline().split('\t')))
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat

"""
函数说明: 求解系数矩阵w
Parameters:
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
Returns:
    ws - 回归系数
"""
def standRegres(xArr, yArr):
    #转换成矩阵
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    #计算x^T*xT
    xTx = xMat.T*xMat
    #判断xTx是否可逆, 如果不可逆则无法计算
    #linalg.det()计算行列式
    if np.linalg.det(xTx) == 0:
        print('矩阵不可逆,无法求解逆矩阵')
        return
    #利用公式,计算w
    ws = xTx.I*(xMat.T*yMat)
    return ws

"""
函数说明: 使用局部加权线性回归计算回归系数w
            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归
Parameters:
    testPoit - 测试样本点
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
    k - 高斯核的k
Returns:
    testPoint * ws - 数据点与具有权重的系数相乘得到的预测点
"""
def lwlr(testPoint, xArr, yArr, k = 1):
    #转换成矩阵
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    #获取xMat矩阵的行数
    m = np.shape(xMat)[0]
    #初始化权重(为对角矩阵)
    #eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重
    weights = np.mat(np.eye((m)))
    #遍历数据集, 计算每个样本的权重
    for j in range(m):
        #testPoint 的形式是 一个行向量的形式
        #计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值
        diffMat = testPoint - xMat[j, :]
        weights[j,j] = np.exp(diffMat*diffMat.T/ (-2*k**2))
    xTx = xMat.T * (weights * xMat)
    if np.linalg.det(xTx) == 0:
        print("矩阵不可逆")
        return
    #计算出回归系数ws
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws

"""
函数说明: 局部加权线性回归测试
Parameters:
    testArr - 测试数据集
    xArr - 输入的样本(x数据集)
    yArr - 输入的对应标签(y数据集)
    k - 高斯核的k
Returns:
    yHat - 预测值
"""
def lwlrTest(testArr, xArr, yArr, k = 1):
    #得到样本点总数
    m = np.shape(testArr)[0]
    #初始化yHat, 构建一个全部是 0 的 1 * m 矩阵
    yHat = np.zeros(m)
    #循环所有数据点, 并应用lwlr函数
    for i in range(m):
        yHat[i] = lwlr(testArr[i], xArr, yArr, k)
    return yHat

"""
函数说明: 返回真实值与预测值误差大小
Parameters:
    yArr - 样本的真实值
    yHatArr - 样本的预测值
Returns:
    误差
"""
def rssError(yArr, yHatArr):
    return ((yArr - yHatArr)**2).sum()

"""
函数说明: 预测鲍鱼的年龄
Parameters:
    无
Returns:
    无
"""
def abaloneTest():
    abX, abY = loadDataSet('CH08/abalone.txt')
    print('训练集与测试集相同: 局部加权线性回归, 核参数k的大小对预测的影响: ')
    #不同的k进行预测
    oldyHat01 = lwlrTest(abX[0:99],abX[0:99], abY[0:99], 0.1)
    oldyHat1 = lwlrTest(abX[0:99],abX[0:99], abY[0:99], 1)
    oldyHat10 = lwlrTest(abX[0:99],abX[0:99], abY[0:99], 10)
    # 打印出不同的核预测值与新数据集(测试数据集)上的真实值之间的误差大小
    print('k=0.1时,误差大小为:', rssError(abY[0:99], oldyHat01.T))
    print('k=1  时,误差大小为:', rssError(abY[0:99], oldyHat1.T))
    print('k=10 时,误差大小为:', rssError(abY[0:99], oldyHat10.T))

    print("训练集与测试集不同: 局部加权线性回归, 和参数k的大小对预测的影响: ")
    newyHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)
    newyHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)
    newyHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)
    print('k=0.1时,误差大小为:', rssError(abY[100:199], newyHat01.T))
    print('k=1  时,误差大小为:', rssError(abY[100:199], newyHat1.T))
    print('k=10 时,误差大小为:', rssError(abY[100:199], newyHat10.T))

    print("训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:")
    print("k=1时,局部加权线性回归的误差大小为: ", rssError(abY[100:199], newyHat1.T))

    ws = standRegres(abX[0:99], abY[0:99])
    linearYhat = np.mat(abX[100:199]) * ws
    #注意, array可以和list计算(结果为array), 但是array和matrix不可以, 因此需要.A转换成array
    print("简单线性回归误差大小为: ", rssError(abY[100:199], linearYhat.T.A))


if __name__ == '__main__':
    abaloneTest()

```

这里还有一点常常遇到的一个东西, 我觉得挺重要的, 就是array,list和matrix之间的计算

<font color=red>**array可以和list计算(结果为array), 但是array和matrix不可以, 因此需要.A转换成array**</font>



运行结果为:

![](https://i.loli.net/2019/09/30/Bpf3YaHmhQ57Nnv.png)







### 四 缩减系数来“理解”数据

如果数据的特征比样本点还多应该怎么办? 是否还可以使用线性回归和之前的方法来做预测? 答案是否定的,  即不能再使用前面介绍的方法.  

这是因为在计算 $\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1}$ 的时候会出错.  如果特征比样本点还多 (n > m),  也就是说输入数据的矩阵X不是满秩矩阵.  非满秩矩阵在求逆时会出现问题.  

为解决这个问题, 统计学家引入了岭回归(ridge regression)的概念, 接着时lasso法, 该方法效果很好但是计算复杂.

#### 1 岭回归

简单说来,  岭回归就是在矩阵 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 上加一个 $\lambda \mathbf{I}$ 从而使得矩阵非奇异,  进而能对 $\mathbf{X}^{\mathrm{T}} \mathbf{X}+\lambda \mathbf{I}$ 求逆.  其中矩阵I是一个m×m的单位矩阵,  对角线上元素全为1,  其他元素全为0.  而λ是一个用户定义的数值,  后面会做介绍.  在这种情况下,  回归系数的计算公式将变成:   
$$
\hat{w}=\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}+\lambda \boldsymbol{I}\right)^{-1} \boldsymbol{X}^{\mathrm{T}} y
$$
岭回归最先用来处理特征数多于样本数的情况,  现在也用于**在估计中加入偏差,  从而得到更好的估计**.  这里通过引入 λ 来限制了所有w之和,  通过引入该惩罚项,  能够减少不重要的参数,  这个技术在统计学中也叫做缩减shrinkage 

**缩减方法可以去掉不重要的参数,  因此能更好地理解数据.  此外,  与简单的线性回归相比,  缩减法能取得更好的预测效果.**   

具体代码如下:

```python
import numpy as np
import matplotlib.pyplot as plt


"""
函数说明: 加载并解析数据
Parameters:
    filename - 文件名
Returns:
    dataMat - 数据矩阵
    labelMat - 数据标签
"""

def loadDataSet(fileName):
    numFeat = len((open(fileName).readline().split('\t')))
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat

def ridgeRegres(xMat, yMat, lam=0.2):
    """
    岭回归
    :param xMat: x数据集
    :param yMat: y数据集
    :param lam: 缩减系数
    :return: ws-回归系数
    """
    xTx = xMat.T * xMat
    #岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异,  进而能对 xTx + λI 求逆
    denom = xTx + np.eye(np.shape(xMat)[1]) * lam
    # 检查行列式是否为零,  即矩阵是否可逆,  行列式为0的话就不可逆,  不为0的话就是可逆
    if np.linalg.det(denom) == 0:
        print("矩阵是奇异矩阵, 不可逆")
        return
    ws = denom.I * (xMat.T * yMat)
    return  ws

def ridgeTest(xArr, yArr):
    """
    岭回归的测试
    :param xArr:x数据集
    :param yArr:y数据集
    :return:wMat - 回归系数矩阵
    """
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    #中心化
    yMean = np.mean(yMat, 0)
    yMat = yMat - yMean
    xMeans = np.mean(xMat, 0)
    xVar = np.var(xMat, 0)
    xMat = (xMat - xMeans) / xVar
    numTests = 30
    wMat = np.zeros((numTests, np.shape(xMat)[1]))
    for i in range(numTests):
        ws = ridgeRegres(xMat, yMat, np.exp(i-10))
        wMat[i,:] = ws.T
    return wMat

def regression_ridge():
    xArr, yArr = loadDataSet('CH08/abalone.txt')
    ridgeWeights = ridgeTest(xArr,yArr)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(ridgeWeights)
    plt.show()

if __name__ == '__main__':
    regression_ridge()
```



运行结果如下:

![](https://i.loli.net/2019/10/03/ZbsToHgMhmituDW.png)

#### 2 套索方法

在增加如下约束时,  普通的最小二乘法回归会得到与岭回归一样的公式: 
$$
\sum_{k=1}^{n} w_{k}^{2} \leqslant \lambda
$$
上式限定了所有回归系数的平方和不能大于 λ .  使用普通的最小二乘法回归在当两个或更多的特征相关时,  可能会得到一个很大的正系数和一个很大的负系数.  正是因为上述限制条件的存在,  使用岭回归可以避免这个问题.  

与岭回归类似,  另一个缩减方法lasso也对回归系数做了限定,  对应的约束条件如下: 
$$
\sum_{k=1}^{n}\left|w_{k}\right| \leqslant \lambda
$$
唯一的不同点在于,  这个约束条件使用绝对值取代了平方和.  虽然约束形式只是稍作变化,  结果却大相径庭: 在 λ 足够小的时候,  一些系数会因此被迫缩减到 0.这个特性可以帮助我们更好地理解数据.  



关于lasso方法,实现起来很复杂,  然后书上有个简单版的, 就是奇纳香逐步回归算法. 它可以得到与lasso差不多的效果. 是一种贪心算法, 即每一步都尽可能减少误差.  一开始,  所有权重都设置为 0,  然后每一步所做的决策是对某个权重增加或减少一个很小的值.  

伪代码如下:

```
数据标准化,  使其分布满足 0 均值 和单位方差
在每轮迭代过程中: 
    设置当前最小误差 lowestError 为正无穷
    对每个特征:
        增大或缩小:
            改变一个系数得到一个新的 w
            计算新 w 下的误差
            如果误差 Error 小于当前最小误差 lowestError: 设置 Wbest 等于当前的 W
        将 W 设置为新的 Wbest
```

虽然说起来简单, 但我不想敲了, 后续再说吧. 



### 五 权衡偏差和方差

任何时候,  一旦发现模型和测量值之间存在差异,  就说出现了误差.  当考虑模型中的“噪声”或者说误差时,  必须考虑其来源.   

- 在处理过程种, 可能会**对复杂的过程进行简化**,  这将导致在模型和测量值之间出现“噪声”或**误差**,  若无法理解数据的真实生成过程,  也会导致差异的发生.   

- 另外,  **测量过程本身也可能产生“噪声”或者问题**.   

  比如: 简单线性回归和局部加权线性回归中的数据ex0.txt是作者通过随机产生的一个数据, 具体生成公式为: y = 3.0 + 1.7x + 0.1sin(30x) + 0.06N(0,1)

  其中, N(0,1)是一个均值为0, 方差为1的正态分布. 对于简单线性回归来说, 拟合的就是 3.0 + 0.7x这部分, 而误差部分就是 0.1sin(30x) + 0.06N(0,1). 而局部加权线性回归通过多组不同的局部权重来找到具有最小误差的解.



下图给出了**训练误差**和**测试误差**的曲线图,  **上面的曲线就是测试误差**,  **下面的曲线是训练误差**.  根据第三小节局部加权线性回归的实验我们知道:  **如果降低核的大小,  那么训练误差将变小**.  从图中来看,  **从左到右就表示了核逐渐减小的过程**. 

![](https://i.loli.net/2019/10/03/KTQbW3EyFs24qDA.png)

一般认为,  上述两种误差由三个部分组成:  **偏差、测量误差和随机噪声**.  在8.2节和8.3节,  我们通过引入了三个越来越小的核来不断增大模型的方差 

在缩减系数来“理解”数据这一节中,  介绍了缩减法,  可以将一些**系数缩减成很小的值或直接缩减为 0** ,  这是一个**增大模型偏差**的例子 (也就是上图的左侧) .  通过把一些特征的回归系数缩减到 0 ,  同时也就减小了模型的复杂度.  例子中有 8 个特征,  消除其中两个后不仅使模型更易理解,  同时还降低了预测误差.  对照上图,  **左侧是参数缩减过于严厉的结果,  而右侧是无缩减的效果.**  

方差是可以度量的.  如果从鲍鱼数据中取一个随机样本集 (例如取其中 100 个数据) 并用线性模型拟合,  将会得到一组回归系数.  同理,  再取出另一组随机样本集并拟合,  将会得到另一组回归系数.  这些系数间的差异大小也就是模型方差的反映.  



# 8.3 使用sklearn实现简单线性回归

sklearn中有比较全的一般线性回归模型, 比如简单线性回归, 岭回归, lasso回归, 贝叶斯回归等等. 这里只简单介绍下简单线性回归LinearRegression模块.

![1570107498045](C:\Users\HeatonHsu\AppData\Roaming\Typora\typora-user-images\1570107498045.png)

## 8.3.1 linear_model.LinearRegression()

![](https://i.loli.net/2019/10/03/8YAH4oBwGugjPZv.png)

**参数说明:**

LinearRegression中的主要参数如下:

- **fit_intercept**:  是否需要截距,  bool类型,  默认为True.  也就是是否求解b

- **normalize**:  是否先进行归一化,  bool类型,  默认为False.  如果为真,  则回归X将在回归之前被归一化.   当fit_intercept设置为False时,  将忽略此参数.   当回归量归一化时,  注意到这使得超参数学习更加鲁棒,  并且几乎不依赖于样本的数量.   相同的属性对标准化数据无效.  然而,  如果你想标准化,  请在调用normalize = False训练估计器之前,  使用preprocessing.StandardScaler处理数据.    

- **copy_X:**  是否复制X数组,  bool类型,  默认为True,  如果为True,  将复制X数组; 否则,  它覆盖原数组X.  

- **n_jobs**:  是否启动所有CPU,  int类型,   默认值为1



**主要方法:**

同时, LinearRegression中还有一些方法, 具体如下:

![](https://i.loli.net/2019/10/03/EamCYxnPI7R8HFp.png)

对于 8.1 小节, 我们可以重写成:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

"""
函数说明: 加载并解析数据
Parameters:
    filename - 文件名
Returns:
    dataMat - 数据矩阵
    labelMat - 数据标签
"""

def loadDataSet(fileName):
    numFeat = len((open(fileName).readline().split('\t')))
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat

def sklearnLinear(xArr, yArr):
    # 转换成矩阵
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    clf = LinearRegression()
    clf.fit(xMat, yMat)
    a = clf.intercept_
    print(a)

if __name__ == '__main__':
    xArr, yArr = loadDataSet('CH08\ex0.txt')
    sklearnLinear(xArr, yArr)
```

sklearnLinear函数中, 我返回了截距项, 即 clf.intercept_

结果如下:

![](https://i.loli.net/2019/10/03/vjCxUFNwyh6zs5Y.png)

可以看出和书上是一致的.





# 最后提一点

关于是否有截距的问题, 如果有截距的话, 我们就把 b 收进w中, 那么向量w就是(w0;w1;....;wd).对应的(x0,x1,....xd).此时, x0=1.  目的就是为了简化操作. 

所以, 可以看到数据集ex0.txt和ex1.txt本身第一列都已经是1了, 直接求解得到w0, 而对于鲍鱼的数据集abalone.txt第一列不是1, 回归得来的就没有截距项, 如果想要截距项, 那么需要对原来的数据进行处理, 第一列设置成全为1.(这是针对书上的算法来说的, 书上就是根据公式来的)



<font color=red>**用书上写的算法, 如果数据集第一列全是1的话, 那么默认是有截距项的, 如果第一列不是1, 那么默认是没有截距项的.** </font>

因为如果全是1, 那么根据公式就知道, w0*x0, x0恒为1, w0是我们算出的参数, 是定值, 所以, w0就是截距项, 

w0=b



<font color=red>**如果用sklearn.linear_model中的LinearRgression模块,  里面有参数 fit_intercept 可以设置有无截距项, 默认为True.**</font>





然后, 我用了分别用书上和sklearn做了验证, 我先对数据进行了处理, 把ex0.txt第一列(这一列全是1)删除, 得到一个新的文件ex0 - 副本.txt.  ex0.txt和ex0 - 副本.txt文件格式分别如下:

![](https://i.loli.net/2019/10/01/YUJXs3qMwimgBkd.png)

1. 先用书上的算法standRegress函数, 结果分别如下:

   ![](https://i.loli.net/2019/10/01/YhOUKr2H9IDg6Wb.png)

   可以看到, ws1就是书上的, 有截距项, 为3.00774324, 而ws2只有一个w, 没有截距项, 画出的图分别如下:

![](https://i.loli.net/2019/10/01/rHOSnW6LvRI3NGx.png)

![](https://i.loli.net/2019/10/01/bD1nrQRokTiEtPa.png)

可以看出差别还是很大的,  所以, 针对书上的standRegress函数(也就是计算ws)来说, 如果数据集实际是有截距项的, 那么第一列必须是1, 不然, 结果很明显是不正确的. 



2. 用sklearn模块, 其中默认是有截距项的, 即参数 *fit_intercept=True* .

   ![](https://i.loli.net/2019/10/01/1bMogxhP5zlXpYs.png)

   

   可以看到, 都可以得到正确的结果, 为什么这样, 我考虑是这样的, 如果默认有截距项, 那么对于ex0 - 副本.txt来说, sklearn在处理时就已经把第一项全为1考虑进去了(即加了第一列全为1), 显然能得到结果; 而对于ex0.txt来说, 也是一样, 也会把第一项全为1考虑进去(也加了第一列全为1), 但是这时原来的第一列变成第二列, 而第二列还时为1, 最后的结果就是w0,w1都是截距项, 合并一起, 肯定时一样的结果了.





总结来说, 书上的已经把截距项收纳进去了, 一起计算出w, 如果原数据集第一列都为1, 那么最后的ws的第一项就是截距项, 第一列不是全为1, 那么ws最后就没有截距项.

而对于sklearn来说, 这些不成问题, 他已经先期处理过了,考虑进去过了

